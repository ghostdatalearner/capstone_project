---
title: "JHU_DS_MilestoneReport"
author: "Ghostdatalearner"
date: "17/03/2015"
output: html_document
---

# Introduction
This document is a report on the preliminary analysis of a collection of text files with the final goal of building a word prediction application. It is part of the capstone project of the John Hopkins University Data Specialization series at Coursera <https://www.coursera.org/specialization/jhudatascience/>. 

# Executive Report
We study in this report the distribution of a sample of unigrams, bigrams and trigrams of the selected texts collection. Each n-gram is a sequence of n words. Building a table of frequencies of the different n-grams is the basis to build a prediction model. Putting it simple, if the sentence we want to predict contains n-1 words, we will choose the most common n-gram with those same n-1 words, and the last one will be the outcome of the prediction. We have found that distributions are more even as the order of the n-gram decreases, and that stop words are very common. As a result, we think that our future prediction model should start with the highest order n-gram available and include corrections to take into account the high frequence of stop words.


# Environment and Data
Analysis has been performed with a personal computer running Windows 7.

The data set is provided by SwiftKey and was downloaded from <https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip>. It contains twelve files, with samples of texts in Finnish, German, English and Russian taken from Twitter, news services and blogs. We have worked with the English files.

## Raw Data Summary
These are the statistics of the three raw files, that we have got with the shell commands ls -l,  wc -l and wc -w

```html
File                Size (bytes)         Lines              Words
=======================================================================
en_US.blogs.txt     210160014             899288           37334114
en_US.news.txt      205811889            1010242           34365936
en_US.twitter.txt   167105338            2360148           30359804

```


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
options(warn=-1)
library(ggplot2)
library(R.utils)
library(stringr)
library(tm)
library(RWeka)
library(ngram)
library(grid)
library(gridExtra)
library(slam)
```

## Sampling and cleaning

As files are rather heavy, we work with sample to make the exploratory analysis. We pick at random 10000 lines of each file and merge them all in a unique corpus. These are the transformations we have applied:

* Unicode chars are removed.
* Punctuation signs, digits and trailing blanks are removed.
* Profanity words are removed.
* The string is converted to lower case.


# Exploratory analysis

We proceed to build the frequency tables of unigrams (i.e. individual words). N-grams have been computed using the function NGramTokenizer of the package RWeka. We have computed two tables, one including all unigrams and one excluding stop words (filtered with the tm_map function). These are the top 30 most frequent unigrams of both classes:

```{r, echo =FALSE, cache = TRUE, fig.height=9,fig.width=9,results='asis'}
set.seed(1234)
filename <- c("en_US.twitter.txt","en_US.news.txt","en_US.blogs.txt")
# This figures are preloaded to avoid counting lines each time the script is run
clines <- c(2360148,1010242,899288)
sampletext <- c()
prof_words <- read.table("en", sep="\n")
samplesize <- 100
top_cut <- 30

read_file <- function(fn)
{
  cl <- clines[which(filename == fn)]
  con <- file(description=fn, open="rb")
  linestoread <- samplesize
  tmp <- readLines(con, n=linestoread) 
  tmp <- iconv(tmp, "latin1", "ASCII", sub="") 
  close(con)
  return(tmp)
}
g <- lapply(filename,read_file)
vc <- VCorpus(VectorSource(g))
vc <- tm_map(vc, removeNumbers) 
vc <- tm_map(vc, stripWhitespace) 
vc <- tm_map(vc, removePunctuation)
vc <- tm_map(vc, tolower)
vc <- tm_map(vc, removeWords, prof_words$V1)
vc_nonstop <- tm_map(vc, removeWords, stopwords('english'))
rm(g)
```


```{r echo =FALSE}
df_corpus <- data.frame(text=unlist(vc),stringsAsFactors=FALSE)
df_corpus_nonstop <- data.frame(text=unlist(vc_nonstop),stringsAsFactors=FALSE)
delimiter_str <- ".!?,; \\t\\r\\n\"()"

histo_ngram <- function(df_corpus,gram_size,gram_name,delimiters,top_cut,color_hist)  
{
  n_words <- NGramTokenizer(df_corpus, Weka_control(min=gram_size,max=gram_size,delimiters = delimiter_str))
  df_ngram <- data.frame(table(n_words))
  df_ngram <- df_ngram[rev(order(df_ngram$Freq)),][1:top_cut,]
  df_ngram <- transform(df_ngram, n_words=reorder(n_words,Freq))
  histo_ngram<- ggplot(df_ngram, aes(n_words,Freq)) + xlab(gram_name) + ylab (paste("Repetitions in",length(filename)*samplesize,"sentences")) +
    geom_bar(width = 0.75, stat="identity",color="white",fill = color_hist) + 
    theme(legend.position = "none") + ggtitle(paste(gram_name,"count")) +
    coord_flip() + theme_bw()
  calc_vals <- list("histo" = histo_ngram, "df_ngram" = df_ngram, "n_words" = n_words) 
  return(calc_vals)
}

v <- histo_ngram(df_corpus,1,"Unigrams",delimiter_str,top_cut,"coral")
histo_unigram <- v[["histo"]]
df_unigram <- v[["df_ngram"]]
uni_words<- v[["n_words"]]

v <- histo_ngram(df_corpus_nonstop,1,"Unigrams excluding stop words",delimiter_str,top_cut,"lightblue")
histo_unigram_nonstop <- v[["histo"]]
df_unigram_nonstop <- v[["df_ngram"]]
uni_words_nonstop<- v[["n_words"]]

v <- histo_ngram(df_corpus,2,"Bigrams",delimiter_str,top_cut,"lavender")
histo_bigram <- v[["histo"]]
df_bigram <- v[["df_ngram"]]
bi_words<- v[["n_words"]]

v <- histo_ngram(df_corpus,3,"Trigrams",delimiter_str,top_cut,"lightgreen")
histo_trigram <- v[["histo"]]
df_trigram <- v[["df_ngram"]]
tri_words<- v[["n_words"]]
```

```{r, echo =FALSE, fig.height=4.5,fig.width=9,results='asis'}
grid.arrange(histo_unigram,histo_unigram_nonstop, ncol=2, widths=c(1/2,1/2))
```
<center><b>Figure 1: Frequency tables of the most common unigrams.</b></center><br>

The barplot shows that, as expected, most repeated unigrams are English stop words. If we strip them the result is quite different.
This procedure is useful for the exploratory analysis but we can't remove all stop words or it may have a disturbing effect in our predictions.

```{r, echo =FALSE, fig.height=4.5,fig.width=9}
grid.arrange(histo_bigram,histo_trigram, ncol=2, widths=c(1/2,1/2))
```
<center><b>Figure 3: Frequency tables of bi and trigrams.</b></center><br>

Bigrams and trigrams show the ubiquity of stop words, but it is also clear how important they are for the meaning of the expressions.

It is interesting to study the density distributions of different n-grams. The following picture shows how these distributions are strongly skewed, but also how different are for unigrams, bigrams and trigrams. Most unigrams appear a very low number of times, so they are poor indicators for prediction. The bigrams distribution is more skewed, and the trigram repetitions are more unevenly repeated. A qualitative explanation is that as the order of the n-gram grows they convey more information, a high number of repetetion of a particular n-gram is a very strong predictor.
On the other hand, if the n-gram order is too long there is a risk of overfitting the model. In the limit we would have only a n-gram with all the words of the corpus, and a null ability to predict. So, it is very important to find a balance between the ammount of information and the bias of the maximum n-gram length of the prediction model.

```{r, echo =FALSE, fig.height=4.5,fig.width=9}
df_unigramtot <- data.frame(table(uni_words))
df_unigramtot$ngram <- "Unigram"
names(df_unigramtot) <- c("n-gram","Freq","ngram")
df_bigramtot <- data.frame(table(bi_words))
df_bigramtot$ngram <- "Bigram"
names(df_bigramtot) <- c("n-gram","Freq","ngram")
df_trigramtot <- data.frame(table(tri_words))
df_trigramtot$ngram <- "Trigram"
names(df_trigramtot) <- c("n-gram","Freq","ngram")
df_tot <- rbind(df_trigramtot,df_bigramtot,df_unigramtot)

d3 <- ggplot(df_tot,aes(x=Freq,fill=ngram,color=ngram)) +geom_density(adjust=15,alpha=0.3) +scale_x_log10()  + theme_bw()
d3

```
<center><b>Figure 4: Density distributions of n-grams.</b></center><br>

# Next steps

* The planned strategy is predicting the next word in a series using the n-gram tables of probability. As n-gram computation is highly time consuming, the approach will be limited to 4-gram. 
* We found problems dealing with memory size when trying to increase the sample size, so a more efficient way to read the raw files must be designed.
* We will use the Katz' back-off and will test different smoothing approaches. We will choose that more efficient in terms of memory and CPU usage, having in mind that the application will be deployed in the Shinyapps server.
* Exploratory analysis shows the high frequence of stop words in the top positions of n-gram tables. Filtering them yielded a quite different table for unigrams, but it is not possible to strip them all without losing prediction accuracy.
